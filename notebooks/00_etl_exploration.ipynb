{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bb7789",
   "metadata": {},
   "source": [
    "11/28/2025 - Nafisa - Load all raw data files\n",
    "**Purpose:**  \n",
    "Load all CSV, JSON, and Parquet files from the fall2025_L directory, merge them into a single DataFrame, clean labels, and save the combined dataset for downstream processing.\n",
    "\n",
    "\n",
    "**Interpretation / Findings:** \n",
    "- Total 5 unique labels found: BENIGN, DoS Hulk, DoS GoldenEye, DoS Slowhttptest, Heartbleed.\n",
    "- Heartbleed (11 rows) removed as required.\n",
    "- Created binary target column Attack → 1 for attacks, 0 for benign.\n",
    "- Final dataset size after cleaning:\n",
    "    -  Attack = 1: 56,112 rows\n",
    "    - Attack = 0: 5,005 rows\n",
    "- Output saved to: data/interm/combined_raw.csv\n",
    "\n",
    "**Notes for Team:**\n",
    "The combined dataset is now ready for feature engineering and model training. Please ensure to review the cleaned labels and the binary target column for consistency before proceeding. \n",
    "\n",
    "### Done with Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8dcojdhc5s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total DOS attacks: 56112\n",
      "Total BENIGN: 5005\n",
      "Total records: 61117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the dataset \n",
    "df = pd.read_csv('../data/interm/combined_raw.csv')\n",
    "\n",
    "# Count each label amount \n",
    "label_counts = df[' Label'].value_counts()\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print()\n",
    "\n",
    "# Count the total amount of DOS attacks \n",
    "total_dos = label_counts[label_counts.index != 'BENIGN'].sum()\n",
    "print(f\"Total DOS attacks: {total_dos}\")\n",
    "print(f\"Total BENIGN: {label_counts['BENIGN']}\")\n",
    "print(f\"Total records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqnn1gg4knn",
   "metadata": {},
   "source": [
    "### Label Distribution\n",
    "\n",
    "| Label | Count |\n",
    "|-------|-------|\n",
    "| DoS Hulk | 30,027 |\n",
    "| DoS GoldenEye | 20,586 |\n",
    "| DoS Slowhttptest | 5,499 |\n",
    "| BENIGN | 5,005 |\n",
    "\n",
    "**Summary:**\n",
    "- Total DOS attacks: 56,112\n",
    "- Total BENIGN: 5,005\n",
    "- Total records: 61,117\n",
    "\n",
    "DONE WITH TASK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124f10a",
   "metadata": {},
   "source": [
    "11/28/2025 **Moosa - Raw Data Overview** \n",
    "\n",
    "This section documents all raw data files stored in `data/raw/fall2025_L/`\n",
    "The goal is to inspect the raw files, determine their formats, load each file once to view their shapes, and provide a brief description of what each file contains.\n",
    "\n",
    "The goal of this task is:\n",
    "- to understand the structure and size of each raw dataset,\n",
    "- to confirm that all files are readable,\n",
    "- and to provide clear documentation for the ETL pipeline before cleaning and processing.\n",
    "\n",
    "Below, I load each file programmatically and print its `(rows, columns)` shape.  \n",
    "After that, I provide markdown summaries for each file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7bb6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: d:\\ML Assignment\\Project\\classifier-DoS-project\\data\\raw\\fall2025_L \n",
      "\n",
      "ids_0.csv → (1001, 79)\n",
      "ids_1.csv → (1001, 79)\n",
      "ids_10.json → (5510, 79)\n",
      "ids_11.parquet → (1025, 79)\n",
      "ids_2.csv → (1001, 79)\n",
      "ids_3.json → (1001, 79)\n",
      "ids_4.json → (1001, 79)\n",
      "ids_5.parquet → (15001, 79)\n",
      "ids_6.parquet → (5001, 79)\n",
      "ids_7.json → (9000, 79)\n",
      "ids_8.parquet → (10293, 79)\n",
      "ids_9.json → (10293, 79)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = \"../data/raw/fall2025_L\"\n",
    "\n",
    "print(\"Looking in:\", os.path.abspath(path), \"\\n\")\n",
    "\n",
    "for f in sorted(os.listdir(path)):\n",
    "    full = os.path.join(path, f)\n",
    "\n",
    "    if f.endswith(\".csv\"):\n",
    "        df = pd.read_csv(full)\n",
    "    elif f.endswith(\".json\"):\n",
    "        df = pd.read_json(full, lines=True)\n",
    "    elif f.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(full)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(f\"{f} → {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac07be",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Total raw files: **12**\n",
    "- Formats used: **CSV (3), JSON (5), Parquet (4)**\n",
    "- All files load successfully with **79 consistent columns**\n",
    "- Only row counts vary between files\n",
    "- These files are the direct inputs to `etl/load_files.py` before merging and cleaning\n",
    "\n",
    "Task Completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925efe3b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
