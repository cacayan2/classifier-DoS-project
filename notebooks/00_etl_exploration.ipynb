{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328493f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV: ids_2.csv\n",
      "Reading CSV: ids_0.csv\n",
      "Reading CSV: ids_1.csv\n",
      "Reading JSON: ids_7.json\n",
      "Reading JSON: ids_10.json\n",
      "Reading JSON: ids_9.json\n",
      "Reading JSON: ids_4.json\n",
      "Reading JSON: ids_3.json\n",
      "Reading Parquet: ids_5.parquet\n",
      "Reading Parquet: ids_6.parquet\n",
      "Reading Parquet: ids_11.parquet\n",
      "Reading Parquet: ids_8.parquet\n",
      "Combined DataFrame shape: (61128, 79)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_all_files(folder_path):\n",
    "    \"\"\"\n",
    "    Read all CSV, JSON, and Parquet files from a folder and combine into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to the folder containing the data files\n",
    "\n",
    "    Returns:\n",
    "        Combined pandas DataFrame\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    dataframes = []\n",
    "\n",
    "    # Read all CSV files\n",
    "    for csv_file in folder.glob(\"*.csv\"):\n",
    "        print(f\"Reading CSV: {csv_file.name}\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Read all JSON files\n",
    "    for json_file in folder.glob(\"*.json\"):\n",
    "        print(f\"Reading JSON: {json_file.name}\")\n",
    "        try:\n",
    "            # Try JSON Lines format first (most common for data files)\n",
    "            tmp_df = pd.read_json(json_file, lines=True)\n",
    "            dataframes.append(tmp_df)\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Unexpected JSON structure in {json_file.name}\")\n",
    "        # tmp_df = pd.read_json(json_file, lines=True)\n",
    "        # dataframes.append(tmp_df)\n",
    "\n",
    "    # Read all Parquet files\n",
    "    for parquet_file in folder.glob(\"*.parquet\"):\n",
    "        print(f\"Reading Parquet: {parquet_file.name}\")\n",
    "        table = pq.read_table(parquet_file)\n",
    "        df = table.to_pandas()\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Combine all dataframes\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No files found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "# Usage\n",
    "folder_path = \"../data/raw/fall2025_L/\"\n",
    "df = read_all_files(folder_path)\n",
    "print(f\"Combined DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcojdhc5s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'label' column:\n",
      "['BENIGN' 'DoS Hulk' 'DoS Slowhttptest' 'Heartbleed' 'DoS GoldenEye']\n",
      "\n",
      "Value counts:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Heartbleed             11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique labels: 5\n"
     ]
    }
   ],
   "source": [
    "# Check if 'labels' column exists\n",
    "if ' Label' in df.columns:\n",
    "    print(\"\\nUnique values in 'label' column:\")\n",
    "    print(df[' Label'].unique())\n",
    "\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(df[' Label'].value_counts())\n",
    "\n",
    "    print(f\"\\nTotal unique labels: {df[' Label'].nunique()}\")\n",
    "else:\n",
    "    print(\"'labels' column not found in the dataframe\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b47c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After removing Heartbleed:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary label distribution:\n",
      "Attack\n",
      "1    56112\n",
      "0     5005\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows with 'Heartbleed' label\n",
    "df = df[df[' Label'] != 'Heartbleed']\n",
    "\n",
    "# Create binary column: 0 for BENIGN, 1 for all attacks\n",
    "df['Attack'] = (df[' Label'] != 'BENIGN').astype(int)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nAfter removing Heartbleed:\")\n",
    "print(df[' Label'].value_counts())\n",
    "\n",
    "print(\"\\nBinary label distribution:\")\n",
    "print(df['Attack'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11acfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving dataframe to ../data/interm/combined_raw.csv...\n",
      "Successfully saved! File size: 22.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Save the combined dataframe to CSV\n",
    "output_path = \"../data/interm/combined_raw.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "print(f\"\\nSaving dataframe to {output_path}...\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\n",
    "    f\"Successfully saved! File size: {Path(output_path).stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb7789",
   "metadata": {},
   "source": [
    "11/28/2025 - Nafisa - Load all raw data files\n",
    "**Purpose:**  \n",
    "Load all CSV, JSON, and Parquet files from the fall2025_L directory, merge them into a single DataFrame, clean labels, and save the combined dataset for downstream processing.\n",
    "\n",
    "\n",
    "**Interpretation / Findings:** \n",
    "- Total 5 unique labels found: BENIGN, DoS Hulk, DoS GoldenEye, DoS Slowhttptest, Heartbleed.\n",
    "- Heartbleed (11 rows) removed as required.\n",
    "- Created binary target column Attack → 1 for attacks, 0 for benign.\n",
    "- Final dataset size after cleaning:\n",
    "    -  Attack = 1: 56,112 rows\n",
    "    - Attack = 0: 5,005 rows\n",
    "- Output saved to: data/interm/combined_raw.csv\n",
    "\n",
    "**Notes for Team:**\n",
    "The combined dataset is now ready for feature engineering and model training. Please ensure to review the cleaned labels and the binary target column for consistency before proceeding. \n",
    "\n",
    "### Done with Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dcojdhc5s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total DOS attacks: 56112\n",
      "Total BENIGN: 5005\n",
      "Total records: 61117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the dataset \n",
    "df = pd.read_csv('../data/interm/combined_raw.csv')\n",
    "\n",
    "# Count each label amount \n",
    "label_counts = df[' Label'].value_counts()\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print()\n",
    "\n",
    "# Count the total amount of DOS attacks \n",
    "total_dos = label_counts[label_counts.index != 'BENIGN'].sum()\n",
    "print(f\"Total DOS attacks: {total_dos}\")\n",
    "print(f\"Total BENIGN: {label_counts['BENIGN']}\")\n",
    "print(f\"Total records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqnn1gg4knn",
   "metadata": {},
   "source": [
    "### Label Distribution\n",
    "\n",
    "| Label | Count |\n",
    "|-------|-------|\n",
    "| DoS Hulk | 30,027 |\n",
    "| DoS GoldenEye | 20,586 |\n",
    "| DoS Slowhttptest | 5,499 |\n",
    "| BENIGN | 5,005 |\n",
    "\n",
    "**Summary:**\n",
    "- Total DOS attacks: 56,112\n",
    "- Total BENIGN: 5,005\n",
    "- Total records: 61,117\n",
    "\n",
    "DONE WITH TASK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a787245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns before formatting:\n",
      "[' Destination Port', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', ' Fwd Packet Length Max', ' Fwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Packet Length Std', 'Bwd Packet Length Max', ' Bwd Packet Length Min', ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size', ' Avg Fwd Segment Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min', ' Label', 'Attack']\n"
     ]
    }
   ],
   "source": [
    "print(\"All columns before formatting:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b073e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns after formatting:\n",
      "['Destination_Port', 'Flow_Duration', 'Total_Fwd_Packets', 'Total_Backward_Packets', 'Total_Length_of_Fwd_Packets', 'Total_Length_of_Bwd_Packets', 'Fwd_Packet_Length_Max', 'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean', 'Fwd_Packet_Length_Std', 'Bwd_Packet_Length_Max', 'Bwd_Packet_Length_Min', 'Bwd_Packet_Length_Mean', 'Bwd_Packet_Length_Std', 'Flow_Bytes/s', 'Flow_Packets/s', 'Flow_IAT_Mean', 'Flow_IAT_Std', 'Flow_IAT_Max', 'Flow_IAT_Min', 'Fwd_IAT_Total', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Fwd_IAT_Max', 'Fwd_IAT_Min', 'Bwd_IAT_Total', 'Bwd_IAT_Mean', 'Bwd_IAT_Std', 'Bwd_IAT_Max', 'Bwd_IAT_Min', 'Fwd_PSH_Flags', 'Bwd_PSH_Flags', 'Fwd_URG_Flags', 'Bwd_URG_Flags', 'Fwd_Header_Length', 'Bwd_Header_Length', 'Fwd_Packets/s', 'Bwd_Packets/s', 'Min_Packet_Length', 'Max_Packet_Length', 'Packet_Length_Mean', 'Packet_Length_Std', 'Packet_Length_Variance', 'FIN_Flag_Count', 'SYN_Flag_Count', 'RST_Flag_Count', 'PSH_Flag_Count', 'ACK_Flag_Count', 'URG_Flag_Count', 'CWE_Flag_Count', 'ECE_Flag_Count', 'Down/Up_Ratio', 'Average_Packet_Size', 'Avg_Fwd_Segment_Size', 'Avg_Bwd_Segment_Size', 'Fwd_Header_Length.1', 'Fwd_Avg_Bytes/Bulk', 'Fwd_Avg_Packets/Bulk', 'Fwd_Avg_Bulk_Rate', 'Bwd_Avg_Bytes/Bulk', 'Bwd_Avg_Packets/Bulk', 'Bwd_Avg_Bulk_Rate', 'Subflow_Fwd_Packets', 'Subflow_Fwd_Bytes', 'Subflow_Bwd_Packets', 'Subflow_Bwd_Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active_Mean', 'Active_Std', 'Active_Max', 'Active_Min', 'Idle_Mean', 'Idle_Std', 'Idle_Max', 'Idle_Min', 'Label', 'Attack']\n"
     ]
    }
   ],
   "source": [
    "# Clean column names: remove leading/trailing spaces and replace spaces with underscores\n",
    "df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "print(\"All columns after formatting:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320477f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 constant columns:\n",
      "  Bwd_PSH_Flags = 0\n",
      "  Fwd_URG_Flags = 0\n",
      "  Bwd_URG_Flags = 0\n",
      "  CWE_Flag_Count = 0\n",
      "  Fwd_Avg_Bytes/Bulk = 0\n",
      "  Fwd_Avg_Packets/Bulk = 0\n",
      "  Fwd_Avg_Bulk_Rate = 0\n",
      "  Bwd_Avg_Bytes/Bulk = 0\n",
      "  Bwd_Avg_Packets/Bulk = 0\n",
      "  Bwd_Avg_Bulk_Rate = 0\n",
      "\n",
      "Shape: (61117, 70)\n"
     ]
    }
   ],
   "source": [
    "# Find and drop all columns with constant values (including all zeros)\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "\n",
    "print(f\"Found {len(constant_columns)} constant columns:\")\n",
    "for col in constant_columns:\n",
    "    print(f\"  {col} = {df[col].iloc[0]}\")\n",
    "\n",
    "# Drop all constant columns\n",
    "df = df.drop(columns=constant_columns)\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb4c9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Checking if these columns have duplicate data \n",
    "print((df['Fwd_Header_Length'] == df['Fwd_Header_Length.1']).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff90a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (61117, 69)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['Fwd_Header_Length.1'])\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4c2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaN values: []\n",
      "Shape: (61006, 69)\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with all NaN values\n",
    "# Since there are no such columns, this will print an empty list\n",
    "all_nan_columns = df.columns[df.isna().all()].tolist()\n",
    "\n",
    "print(f\"Columns with all NaN values: {all_nan_columns}\")\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "df = df.dropna(axis=0, how='any')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b819e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/cleaned/wednesday_cleaned.csv - Shape: (61006, 69)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "output_path = \"../data/cleaned/wednesday_cleaned.csv\"\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path} - Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0547c",
   "metadata": {},
   "source": [
    "11/29/2025 - Nafisa - Clean Dataset\n",
    "\n",
    "**Purpose:**  \n",
    "Clean the raw Wednesday network traffic dataset by standardizing column names, removing irrelevant features, filtering malicious traffic types, and preparing data for machine learning models.\n",
    "\n",
    "**Interpretation / Findings:** \n",
    "- **Original dataset:** 61,117 rows × 80 columns\n",
    "- **Standardized column names:** Removed leading/trailing spaces, replaced spaces with underscores\n",
    "- **Removed 10 constant columns** (all zeros): Bwd_PSH_Flags, Fwd_URG_Flags, Bwd_URG_Flags, CWE_Flag_Count, Fwd_Avg_Bytes/Bulk, Fwd_Avg_Packets/Bulk, Fwd_Avg_Bulk_Rate, Bwd_Avg_Bytes/Bulk, Bwd_Avg_Packets/Bulk, Bwd_Avg_Bulk_Rate\n",
    "- **Removed 1 duplicate column:** Fwd_Header_Length.1 (identical to Fwd_Header_Length)\n",
    "- **Removed 111 rows** containing NaN values to ensure complete data for ML\n",
    "- **Cleaned dataset:** 61,006 rows × 69 columns\n",
    "\n",
    "**Notes for Team:**\n",
    "- All features are now numeric except Label column\n",
    "- Saved to: `data/cleaned/wednesday_cleaned.csv`\n",
    "\n",
    "### Done with Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6iycvxtcx92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (61006, 69)\n",
      "Total columns: 69\n",
      "Total rows: 61006\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df_clean = pd.read_csv('../data/cleaned/wednesday_cleaned.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df_clean.shape}\")\n",
    "print(f\"Total columns: {df_clean.shape[1]}\")\n",
    "print(f\"Total rows: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1yflxtjs2gb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count (not including labels): 67\n",
      "After cleaning: 69 columns total\n",
      "Features: 67\n"
     ]
    }
   ],
   "source": [
    "# Validate feature count \n",
    "feature_cols = [col for col in df_clean.columns if col not in ['Label', 'Attack']]\n",
    "print(f\"Feature count (not including labels): {len(feature_cols)}\")\n",
    "\n",
    "# got 67 features after previous cleaning \n",
    "# This is becae of removal of constant columns and duplicates\n",
    "print(f\"After cleaning: {df_clean.shape[1]} columns total\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "m64tbzwgip9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Data Types:\n",
      "Destination_Port                 int64\n",
      "Flow_Duration                    int64\n",
      "Total_Fwd_Packets                int64\n",
      "Total_Backward_Packets           int64\n",
      "Total_Length_of_Fwd_Packets      int64\n",
      "                                ...   \n",
      "Idle_Std                       float64\n",
      "Idle_Max                         int64\n",
      "Idle_Min                         int64\n",
      "Label                           object\n",
      "Attack                           int64\n",
      "Length: 69, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#data types for all columns\n",
    "print(\"Column Data Types:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "v0jl63duk2p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Numeric Columns:\n",
      "       Destination_Port  Flow_Duration  Total_Fwd_Packets  \\\n",
      "count      61006.000000   6.100600e+04       61006.000000   \n",
      "mean         761.782857   4.188937e+07           5.866521   \n",
      "std         5745.506930   4.263149e+07          70.992723   \n",
      "min            0.000000   0.000000e+00           1.000000   \n",
      "25%           80.000000   1.734990e+05           2.000000   \n",
      "50%           80.000000   1.197342e+07           6.000000   \n",
      "75%           80.000000   8.577016e+07           8.000000   \n",
      "max        63276.000000   1.199919e+08       17487.000000   \n",
      "\n",
      "       Total_Backward_Packets  Total_Length_of_Fwd_Packets  \\\n",
      "count            61006.000000                 61006.000000   \n",
      "mean                 4.246369                   384.770482   \n",
      "std                 95.562989                  1811.178876   \n",
      "min                  0.000000                     0.000000   \n",
      "25%                  0.000000                     0.000000   \n",
      "50%                  5.000000                   337.000000   \n",
      "75%                  6.000000                   398.000000   \n",
      "max              23539.000000                278248.000000   \n",
      "\n",
      "       Total_Length_of_Bwd_Packets  Fwd_Packet_Length_Max  \\\n",
      "count                 6.100600e+04           61006.000000   \n",
      "mean                  7.267911e+03             261.007180   \n",
      "std                   2.205481e+05             314.820081   \n",
      "min                   0.000000e+00               0.000000   \n",
      "25%                   0.000000e+00               0.000000   \n",
      "50%                   7.216000e+03             326.000000   \n",
      "75%                   1.159500e+04             374.000000   \n",
      "max                   5.440000e+07           20440.000000   \n",
      "\n",
      "       Fwd_Packet_Length_Min  Fwd_Packet_Length_Mean  Fwd_Packet_Length_Std  \\\n",
      "count           61006.000000            61006.000000           61006.000000   \n",
      "mean               12.483313               61.793047             102.165601   \n",
      "std               126.996351              144.867286             119.429763   \n",
      "min                 0.000000                0.000000               0.000000   \n",
      "25%                 0.000000                0.000000               0.000000   \n",
      "50%                 0.000000               47.000000             123.432618   \n",
      "75%                 0.000000               68.571429             156.855256   \n",
      "max              1983.000000             4352.085714            5185.441640   \n",
      "\n",
      "       ...  min_seg_size_forward   Active_Mean    Active_Std    Active_Max  \\\n",
      "count  ...          61006.000000  6.100600e+04  6.100600e+04  6.100600e+04   \n",
      "mean   ...             29.676229  4.299495e+05  3.199975e+04  4.565032e+05   \n",
      "std    ...              5.514102  1.588669e+06  4.122063e+05  1.678732e+06   \n",
      "min    ...              0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    ...             32.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    ...             32.000000  5.700000e+02  0.000000e+00  5.700000e+02   \n",
      "75%    ...             32.000000  1.997000e+03  0.000000e+00  1.997000e+03   \n",
      "max    ...             60.000000  1.420000e+07  1.010000e+07  2.630000e+07   \n",
      "\n",
      "         Active_Min     Idle_Mean      Idle_Std      Idle_Max      Idle_Min  \\\n",
      "count  6.100600e+04  6.100600e+04  6.100600e+04  6.100600e+04  6.100600e+04   \n",
      "mean   4.047084e+05  3.677194e+07  9.959965e+05  3.775662e+07  3.596456e+07   \n",
      "std    1.556882e+06  4.274541e+07  5.291343e+06  4.280190e+07  4.304126e+07   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    4.835000e+02  6.901146e+06  0.000000e+00  6.901146e+06  6.866689e+06   \n",
      "75%    1.990000e+03  8.510000e+07  0.000000e+00  8.530000e+07  8.510000e+07   \n",
      "max    1.370000e+07  1.200000e+08  6.080000e+07  1.200000e+08  1.200000e+08   \n",
      "\n",
      "             Attack  \n",
      "count  61006.000000  \n",
      "mean       0.918008  \n",
      "std        0.274355  \n",
      "min        0.000000  \n",
      "25%        1.000000  \n",
      "50%        1.000000  \n",
      "75%        1.000000  \n",
      "max        1.000000  \n",
      "\n",
      "[8 rows x 68 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/usid/anaconda3/envs/ml/lib/python3.11/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/Users/usid/anaconda3/envs/ml/lib/python3.11/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "#summary statistics\n",
    "print(\"Summary Statistics for Numeric Columns:\")\n",
    "print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eja3g3aiu6r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA Values Per Column:\n",
      "No NA values found in any column\n",
      "\n",
      "Total NA values in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for NA values\n",
    "print(\"NA Values Per Column:\")\n",
    "na_counts = df_clean.isna().sum()\n",
    "na_percent = (na_counts / len(df_clean)) * 100\n",
    "\n",
    "na_summary = pd.DataFrame({\n",
    "    'NA_Count': na_counts,\n",
    "    'NA_Percent': na_percent\n",
    "})\n",
    "\n",
    "# Only show columns with NA values\n",
    "na_summary_filtered = na_summary[na_summary['NA_Count'] > 0]\n",
    "\n",
    "if len(na_summary_filtered) > 0:\n",
    "    print(na_summary_filtered)\n",
    "else:\n",
    "    print(\"No NA values found in any column\")\n",
    "    \n",
    "print(f\"\\nTotal NA values in dataset: {na_counts.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o8fdjrgj5il",
   "metadata": {},
   "source": [
    "11/29/2025 - Umar - Validate Cleaned Dataset\n",
    "\n",
    "**Purpose:**  \n",
    "Validate the cleaned Wednesday dataset by verifying feature count, checking data types, reviewing summary statistics, and confirming no missing values remain.\n",
    "\n",
    "**Interpretation / Findings:** \n",
    "- **Feature Count:** 67 features excluding label column and attack column\n",
    "  - After Nafisa's cleaning process removed 11 columns we have 67 features\n",
    "  - Total columns: 69 (67 features + 2 label columns)\n",
    "- **Data Types:** All feature columns are numeric (int64 or float64), Label and Attack columns are object/int types\n",
    "- **Summary Statistics:** calculated statistics for all numeric columns including mean, std, min, max, and quartiles\n",
    "- **Missing Values:** Confirmed zero NA values in all columns after Nafisa's cleaning process removed rows with missing data\n",
    "- **Dataset Quality:** Clean dataset has no missing values and all numeric features and is ready for next steps \n",
    "\n",
    "**Notes for Team:**\n",
    "The cleaned dataset has been validated and is ready for next steps \n",
    "\n",
    "### Done with Task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
