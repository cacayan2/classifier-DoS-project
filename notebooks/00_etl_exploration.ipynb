{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328493f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV: ids_2.csv\n",
      "Reading CSV: ids_0.csv\n",
      "Reading CSV: ids_1.csv\n",
      "Reading JSON: ids_7.json\n",
      "Reading JSON: ids_10.json\n",
      "Reading JSON: ids_9.json\n",
      "Reading JSON: ids_4.json\n",
      "Reading JSON: ids_3.json\n",
      "Reading Parquet: ids_5.parquet\n",
      "Reading Parquet: ids_6.parquet\n",
      "Reading Parquet: ids_11.parquet\n",
      "Reading Parquet: ids_8.parquet\n",
      "Combined DataFrame shape: (61128, 79)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_all_files(folder_path):\n",
    "    \"\"\"\n",
    "    Read all CSV, JSON, and Parquet files from a folder and combine into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to the folder containing the data files\n",
    "\n",
    "    Returns:\n",
    "        Combined pandas DataFrame\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    dataframes = []\n",
    "\n",
    "    # Read all CSV files\n",
    "    for csv_file in folder.glob(\"*.csv\"):\n",
    "        print(f\"Reading CSV: {csv_file.name}\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Read all JSON files\n",
    "    for json_file in folder.glob(\"*.json\"):\n",
    "        print(f\"Reading JSON: {json_file.name}\")\n",
    "        try:\n",
    "            # Try JSON Lines format first (most common for data files)\n",
    "            tmp_df = pd.read_json(json_file, lines=True)\n",
    "            dataframes.append(tmp_df)\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Unexpected JSON structure in {json_file.name}\")\n",
    "        # tmp_df = pd.read_json(json_file, lines=True)\n",
    "        # dataframes.append(tmp_df)\n",
    "\n",
    "    # Read all Parquet files\n",
    "    for parquet_file in folder.glob(\"*.parquet\"):\n",
    "        print(f\"Reading Parquet: {parquet_file.name}\")\n",
    "        table = pq.read_table(parquet_file)\n",
    "        df = table.to_pandas()\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Combine all dataframes\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No files found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "# Usage\n",
    "folder_path = \"../data/raw/fall2025_L/\"\n",
    "df = read_all_files(folder_path)\n",
    "print(f\"Combined DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97158edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'label' column:\n",
      "['BENIGN' 'DoS Hulk' 'DoS Slowhttptest' 'Heartbleed' 'DoS GoldenEye']\n",
      "\n",
      "Value counts:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Heartbleed             11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique labels: 5\n"
     ]
    }
   ],
   "source": [
    "# Check if 'labels' column exists\n",
    "if ' Label' in df.columns:\n",
    "    print(\"\\nUnique values in 'label' column:\")\n",
    "    print(df[' Label'].unique())\n",
    "\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(df[' Label'].value_counts())\n",
    "\n",
    "    print(f\"\\nTotal unique labels: {df[' Label'].nunique()}\")\n",
    "else:\n",
    "    print(\"'labels' column not found in the dataframe\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b47c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After removing Heartbleed:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary label distribution:\n",
      "Attack\n",
      "1    56112\n",
      "0     5005\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows with 'Heartbleed' label\n",
    "df = df[df[' Label'] != 'Heartbleed']\n",
    "\n",
    "# Create binary column: 0 for BENIGN, 1 for all attacks\n",
    "df['Attack'] = (df[' Label'] != 'BENIGN').astype(int)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nAfter removing Heartbleed:\")\n",
    "print(df[' Label'].value_counts())\n",
    "\n",
    "print(\"\\nBinary label distribution:\")\n",
    "print(df['Attack'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11acfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving dataframe to ../data/interm/combined_raw.csv...\n",
      "Successfully saved! File size: 22.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Save the combined dataframe to CSV\n",
    "output_path = \"../data/interm/combined_raw.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "print(f\"\\nSaving dataframe to {output_path}...\")\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\n",
    "    f\"Successfully saved! File size: {Path(output_path).stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb7789",
   "metadata": {},
   "source": [
    "11/28/2025 - Nafisa - Load all raw data files\n",
    "**Purpose:**  \n",
    "Load all CSV, JSON, and Parquet files from the fall2025_L directory, merge them into a single DataFrame, clean labels, and save the combined dataset for downstream processing.\n",
    "\n",
    "\n",
    "**Interpretation / Findings:** \n",
    "- Total 5 unique labels found: BENIGN, DoS Hulk, DoS GoldenEye, DoS Slowhttptest, Heartbleed.\n",
    "- Heartbleed (11 rows) removed as required.\n",
    "- Created binary target column Attack → 1 for attacks, 0 for benign.\n",
    "- Final dataset size after cleaning:\n",
    "    -  Attack = 1: 56,112 rows\n",
    "    - Attack = 0: 5,005 rows\n",
    "- Output saved to: data/interm/combined_raw.csv\n",
    "\n",
    "**Notes for Team:**\n",
    "The combined dataset is now ready for feature engineering and model training. Please ensure to review the cleaned labels and the binary target column for consistency before proceeding. \n",
    "\n",
    "### Done with Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dcojdhc5s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts:\n",
      " Label\n",
      "DoS Hulk            30027\n",
      "DoS GoldenEye       20586\n",
      "DoS Slowhttptest     5499\n",
      "BENIGN               5005\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total DOS attacks: 56112\n",
      "Total BENIGN: 5005\n",
      "Total records: 61117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the dataset \n",
    "df = pd.read_csv('../data/interm/combined_raw.csv')\n",
    "\n",
    "# Count each label amount \n",
    "label_counts = df[' Label'].value_counts()\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "print()\n",
    "\n",
    "# Count the total amount of DOS attacks \n",
    "total_dos = label_counts[label_counts.index != 'BENIGN'].sum()\n",
    "print(f\"Total DOS attacks: {total_dos}\")\n",
    "print(f\"Total BENIGN: {label_counts['BENIGN']}\")\n",
    "print(f\"Total records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqnn1gg4knn",
   "metadata": {},
   "source": [
    "### Label Distribution\n",
    "\n",
    "| Label | Count |\n",
    "|-------|-------|\n",
    "| DoS Hulk | 30,027 |\n",
    "| DoS GoldenEye | 20,586 |\n",
    "| DoS Slowhttptest | 5,499 |\n",
    "| BENIGN | 5,005 |\n",
    "\n",
    "**Summary:**\n",
    "- Total DOS attacks: 56,112\n",
    "- Total BENIGN: 5,005\n",
    "- Total records: 61,117\n",
    "\n",
    "DONE WITH TASK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a787245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns before formatting:\n",
      "[' Destination Port', ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', ' Fwd Packet Length Max', ' Fwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Packet Length Std', 'Bwd Packet Length Max', ' Bwd Packet Length Min', ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size', ' Avg Fwd Segment Size', ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward', ' Init_Win_bytes_backward', ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min', ' Label', 'Attack']\n"
     ]
    }
   ],
   "source": [
    "print(\"All columns before formatting:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b073e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns after formatting:\n",
      "['Destination_Port', 'Flow_Duration', 'Total_Fwd_Packets', 'Total_Backward_Packets', 'Total_Length_of_Fwd_Packets', 'Total_Length_of_Bwd_Packets', 'Fwd_Packet_Length_Max', 'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean', 'Fwd_Packet_Length_Std', 'Bwd_Packet_Length_Max', 'Bwd_Packet_Length_Min', 'Bwd_Packet_Length_Mean', 'Bwd_Packet_Length_Std', 'Flow_Bytes/s', 'Flow_Packets/s', 'Flow_IAT_Mean', 'Flow_IAT_Std', 'Flow_IAT_Max', 'Flow_IAT_Min', 'Fwd_IAT_Total', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Fwd_IAT_Max', 'Fwd_IAT_Min', 'Bwd_IAT_Total', 'Bwd_IAT_Mean', 'Bwd_IAT_Std', 'Bwd_IAT_Max', 'Bwd_IAT_Min', 'Fwd_PSH_Flags', 'Bwd_PSH_Flags', 'Fwd_URG_Flags', 'Bwd_URG_Flags', 'Fwd_Header_Length', 'Bwd_Header_Length', 'Fwd_Packets/s', 'Bwd_Packets/s', 'Min_Packet_Length', 'Max_Packet_Length', 'Packet_Length_Mean', 'Packet_Length_Std', 'Packet_Length_Variance', 'FIN_Flag_Count', 'SYN_Flag_Count', 'RST_Flag_Count', 'PSH_Flag_Count', 'ACK_Flag_Count', 'URG_Flag_Count', 'CWE_Flag_Count', 'ECE_Flag_Count', 'Down/Up_Ratio', 'Average_Packet_Size', 'Avg_Fwd_Segment_Size', 'Avg_Bwd_Segment_Size', 'Fwd_Header_Length.1', 'Fwd_Avg_Bytes/Bulk', 'Fwd_Avg_Packets/Bulk', 'Fwd_Avg_Bulk_Rate', 'Bwd_Avg_Bytes/Bulk', 'Bwd_Avg_Packets/Bulk', 'Bwd_Avg_Bulk_Rate', 'Subflow_Fwd_Packets', 'Subflow_Fwd_Bytes', 'Subflow_Bwd_Packets', 'Subflow_Bwd_Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active_Mean', 'Active_Std', 'Active_Max', 'Active_Min', 'Idle_Mean', 'Idle_Std', 'Idle_Max', 'Idle_Min', 'Label', 'Attack']\n"
     ]
    }
   ],
   "source": [
    "# Clean column names: remove leading/trailing spaces and replace spaces with underscores\n",
    "df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "print(\"All columns after formatting:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320477f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 constant columns:\n",
      "  Bwd_PSH_Flags = 0\n",
      "  Fwd_URG_Flags = 0\n",
      "  Bwd_URG_Flags = 0\n",
      "  CWE_Flag_Count = 0\n",
      "  Fwd_Avg_Bytes/Bulk = 0\n",
      "  Fwd_Avg_Packets/Bulk = 0\n",
      "  Fwd_Avg_Bulk_Rate = 0\n",
      "  Bwd_Avg_Bytes/Bulk = 0\n",
      "  Bwd_Avg_Packets/Bulk = 0\n",
      "  Bwd_Avg_Bulk_Rate = 0\n",
      "\n",
      "Shape: (61117, 70)\n"
     ]
    }
   ],
   "source": [
    "# Find and drop all columns with constant values (including all zeros)\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "\n",
    "print(f\"Found {len(constant_columns)} constant columns:\")\n",
    "for col in constant_columns:\n",
    "    print(f\"  {col} = {df[col].iloc[0]}\")\n",
    "\n",
    "# Drop all constant columns\n",
    "df = df.drop(columns=constant_columns)\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb4c9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Checking if these columns have duplicate data \n",
    "print((df['Fwd_Header_Length'] == df['Fwd_Header_Length.1']).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff90a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (61117, 69)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['Fwd_Header_Length.1'])\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4c2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaN values: []\n",
      "Shape: (61006, 69)\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with all NaN values\n",
    "# Since there are no such columns, this will print an empty list\n",
    "all_nan_columns = df.columns[df.isna().all()].tolist()\n",
    "\n",
    "print(f\"Columns with all NaN values: {all_nan_columns}\")\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "df = df.dropna(axis=0, how='any')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b819e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/cleaned/wednesday_cleaned.csv - Shape: (61006, 69)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "output_path = \"../data/cleaned/wednesday_cleaned.csv\"\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path} - Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0547c",
   "metadata": {},
   "source": [
    "11/29/2025 - Nafisa - Clean Dataset\n",
    "\n",
    "**Purpose:**  \n",
    "Clean the raw Wednesday network traffic dataset by standardizing column names, removing irrelevant features, filtering malicious traffic types, and preparing data for machine learning models.\n",
    "\n",
    "**Interpretation / Findings:** \n",
    "- **Original dataset:** 61,117 rows × 80 columns\n",
    "- **Standardized column names:** Removed leading/trailing spaces, replaced spaces with underscores\n",
    "- **Removed 10 constant columns** (all zeros): Bwd_PSH_Flags, Fwd_URG_Flags, Bwd_URG_Flags, CWE_Flag_Count, Fwd_Avg_Bytes/Bulk, Fwd_Avg_Packets/Bulk, Fwd_Avg_Bulk_Rate, Bwd_Avg_Bytes/Bulk, Bwd_Avg_Packets/Bulk, Bwd_Avg_Bulk_Rate\n",
    "- **Removed 1 duplicate column:** Fwd_Header_Length.1 (identical to Fwd_Header_Length)\n",
    "- **Removed 111 rows** containing NaN values to ensure complete data for ML\n",
    "- **Cleaned dataset:** 61,006 rows × 69 columns\n",
    "\n",
    "**Notes for Team:**\n",
    "- All features are now numeric except Label column\n",
    "- Saved to: `data/cleaned/wednesday_cleaned.csv`\n",
    "\n",
    "### Done with Task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
